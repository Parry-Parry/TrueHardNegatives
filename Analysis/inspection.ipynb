{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ir_datasets\n",
      "  Downloading ir_datasets-0.5.5-py3-none-any.whl (318 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.0/318.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting warc3-wet>=0.2.3\n",
      "  Using cached warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
      "Collecting ijson>=3.1.3\n",
      "  Downloading ijson-3.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.9/111.9 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting inscriptis>=2.2.0\n",
      "  Downloading inscriptis-2.3.2-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.38.0 in /home/andrew/anaconda3/lib/python3.9/site-packages (from ir_datasets) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/andrew/anaconda3/lib/python3.9/site-packages (from ir_datasets) (6.0)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /home/andrew/anaconda3/lib/python3.9/site-packages (from ir_datasets) (1.21.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /home/andrew/anaconda3/lib/python3.9/site-packages (from ir_datasets) (4.11.1)\n",
      "Collecting unlzw3>=0.2.1\n",
      "  Using cached unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
      "Collecting pyautocorpus>=0.1.1\n",
      "  Downloading pyautocorpus-0.1.9-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting warc3-wet-clueweb09>=0.2.5\n",
      "  Using cached warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting trec-car-tools>=2.5.4\n",
      "  Using cached trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Collecting lz4>=3.1.10\n",
      "  Downloading lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting zlib-state>=0.1.3\n",
      "  Downloading zlib_state-0.1.5-cp39-cp39-manylinux2010_x86_64.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /home/andrew/anaconda3/lib/python3.9/site-packages (from ir_datasets) (2.28.1)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /home/andrew/anaconda3/lib/python3.9/site-packages (from ir_datasets) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/andrew/anaconda3/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/andrew/anaconda3/lib/python3.9/site-packages (from requests>=2.22.0->ir_datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrew/anaconda3/lib/python3.9/site-packages (from requests>=2.22.0->ir_datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrew/anaconda3/lib/python3.9/site-packages (from requests>=2.22.0->ir_datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrew/anaconda3/lib/python3.9/site-packages (from requests>=2.22.0->ir_datasets) (2022.9.14)\n",
      "Collecting cbor>=1.0.0\n",
      "  Using cached cbor-1.0.0.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: warc3-wet-clueweb09, cbor\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18920 sha256=abc96898d80eea69ac88042bb8644bc3b8f8e80f7fb4a24c833ed87076b8d87a\n",
      "  Stored in directory: /home/andrew/.cache/pip/wheels/7f/22/ed/a11944d7fdf4e94c4206a3f760d385122a4d34d8acc12f71a3\n",
      "  Building wheel for cbor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cbor: filename=cbor-1.0.0-cp39-cp39-linux_x86_64.whl size=23424 sha256=88df57017887e86aa61cba705f1774d16c81390542bc71794024e5cef0228575\n",
      "  Stored in directory: /home/andrew/.cache/pip/wheels/ec/10/03/a281e0682ddd4b310431fb25d1a4f53987105267cf46c417f3\n",
      "Successfully built warc3-wet-clueweb09 cbor\n",
      "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, pyautocorpus, lz4, inscriptis, ir_datasets\n",
      "  Attempting uninstall: lz4\n",
      "    Found existing installation: lz4 3.1.3\n",
      "    Uninstalling lz4-3.1.3:\n",
      "      Successfully uninstalled lz4-3.1.3\n",
      "Successfully installed cbor-1.0.0 ijson-3.2.2 inscriptis-2.3.2 ir_datasets-0.5.5 lz4-4.3.2 pyautocorpus-0.1.9 trec-car-tools-2.6 unlzw3-0.2.2 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load(\"msmarco-passage/eval/small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] If you have a local copy of https://msmarco.blob.core.windows.net/msmarcoranking/top1000.eval.tar.gz, you can symlink it here to avoid downloading it again: /home/andrew/.ir_datasets/downloads/73778cd99f6e0632d12d0b5731b20a02\n",
      "[INFO] [starting] https://msmarco.blob.core.windows.net/msmarcoranking/top1000.eval.tar.gz\n",
      "[INFO] [finished] https://msmarco.blob.core.windows.net/msmarcoranking/top1000.eval.tar.gz: [06:00] [673MB] [1.87MB/s]\n",
      "[INFO] [starting] extracting QID/PID pairs                                                          \n",
      "[INFO] [finished] extracting QID/PID pairs: [00:11] [6515736pair] [549938.61pair/s]\n"
     ]
    }
   ],
   "source": [
    "qrels = pd.DataFrame(dataset.scoreddocs_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_hardest_negatives(scored):\n",
    "    groups = scored.groupby(\"query_id\")\n",
    "    \n",
    "    negatives = defaultdict(list)\n",
    "\n",
    "    for group in groups:\n",
    "        while True:\n",
    "            for row in group.itertuples():\n",
    "                if row.relevance == 3:\n",
    "                    break\n",
    "                else:\n",
    "                    negatives[row.query_id].append(row.doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hard_negatives(scored, cutoff=25):\n",
    "    groups = scored.groupby(\"query_id\")\n",
    "    \n",
    "    negatives = defaultdict(list)\n",
    "\n",
    "    for group in groups:\n",
    "        while True:\n",
    "            for i, row in enumerate(group.itertuples()):\n",
    "                if i == cutoff:\n",
    "                    break\n",
    "                elif row.relevance == 3:\n",
    "                    pass \n",
    "                else:\n",
    "                    negatives[row.query_id].append(row.doc_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
